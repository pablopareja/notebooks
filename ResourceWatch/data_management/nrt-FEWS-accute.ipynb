{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-19T10:36:09.817799",
     "start_time": "2017-04-19T10:36:07.971601"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules first\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import fiona\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo list:  \n",
    "- [x] Merge all .shp into one\n",
    "- [ ] Creates a docker that can add new data to the .shp checking if there is new data available (update check once a month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Merge FEWS Acute datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- [x] read files extract dates\n",
    "- [x] merge them onto a single file and add the date and region to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T10:07:26.736253",
     "start_time": "2017-04-17T10:07:26.668772"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/proj-test/ResourceWatch'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.path.exists(\"/home/jovyan/work/data/rw/data_source/FEWS/caribbean-central-america201301/caribbeancentralamerica201301_ML1.shp\"))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T10:07:26.781470",
     "start_time": "2017-04-17T10:07:26.740765"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t=os.walk(\"/home/jovyan/work/data/rw/data_source/FEWS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T10:07:26.849560",
     "start_time": "2017-04-17T10:07:26.788259"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fileList(dataPath):\n",
    "    shpList=[]\n",
    "    for root, dirs, files in os.walk(dataPath):\n",
    "        for file in files:\n",
    "            if file.endswith(\".shp\"):\n",
    "                text = file.split('.')[0].split('_')\n",
    "                if len(text)>2:\n",
    "                    shpList.append({'path': os.path.join(root, file), 'stype': text[2], 'date': datetime.date(int(text[1][0:4]), int(text[1][4:6]), 1).isoformat(), 'region':text[0]})\n",
    "                else:\n",
    "                    shpList.append({'path': os.path.join(root, file), 'stype': text[1], 'date': datetime.date(int(text[0][-6::][0:4]),int(text[0][-6::][4:6]), 1).isoformat(), 'region':text[0][0:-6]})\n",
    "    return shpList            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T10:21:22.391454",
     "start_time": "2017-04-17T10:21:22.367022"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def manage_shp(path, stype, date, region):\n",
    "    '''\n",
    "    takes a shapefile modifies its structure and gives light into new structure\n",
    "    path: shapefile path\n",
    "    stype: ml1 or ml2\n",
    "    date: YYYY-MM-DD iso format\n",
    "    region: region\n",
    "    '''\n",
    "    newdata = gpd.GeoDataFrame(columns=['geometry', 'value', 'type','date','region'], crs = fiona.crs.from_epsg(4326))\n",
    "    dataset = gpd.read_file(path)\n",
    "    for index, rows in dataset.iterrows():\n",
    "        newdata.loc[index, 'geometry']=rows['geometry'].simplify(0.04,True)\n",
    "        newdata.loc[index,'value']=rows[0]\n",
    "        newdata.loc[index,'type']=stype\n",
    "        newdata.loc[index,'date']=date\n",
    "        newdata.loc[index,'region']=region\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T10:21:23.562251",
     "start_time": "2017-04-17T10:21:23.547836"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    dfList=[]\n",
    "    outfile ='/home/jovyan/work/data/rw/dst/fws.shp'\n",
    "    shpList=fileList('/home/jovyan/work/data/rw/data_source/FEWS/')\n",
    "    print(\"starting ...\")\n",
    "    for shp in shpList:\n",
    "        dfList.append(manage_shp(shp['path'],shp['stype'],shp['date'],shp['region']))\n",
    "    print(\"merging ...\")\n",
    "    dst = pd.concat(dfList)\n",
    "    dst.to_file(outfile)\n",
    "    print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T10:24:53.987949",
     "start_time": "2017-04-17T10:21:26.784116"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting ...\n",
      "merging ...\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-07T11:09:12.227578",
     "start_time": "2017-04-07T11:09:12.210967"
    }
   },
   "source": [
    "# Planet pulse script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- [x] Check for new data\n",
    "- [x] Download it\n",
    "- [x] uncompress\n",
    "- [x] concat them with the last data\n",
    "- [x] compress\n",
    "- [x] upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tinys3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5c409568a98a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtinys3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tinys3'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import requests\n",
    "import tinys3\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import fiona\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zipDir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))\n",
    "            os.remove(file)\n",
    "\n",
    "def downloadFile(url, path):\n",
    "    local_filename = path + url.split('/')[-1]\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "# Download big file\n",
    "def getFwsFile(path):\n",
    "    zipFiles=[]\n",
    "\n",
    "    url = 'https://rw-nrt-scripts.s3.amazonaws.com/fws.zip'\n",
    "    zipFiles.append(downloadFile(url, path))\n",
    "\n",
    "    for file in zipFiles:\n",
    "        with zipfile.ZipFile(file,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(path)\n",
    "        os.remove(file)\n",
    "\n",
    "# Download fews files\n",
    "def getFiles(date, path):\n",
    "    regions = ['west-africa','southern-africa','central-asia','east-africa','caribbean-central-america']\n",
    "    zipFiles=[]\n",
    "    for region in regions:\n",
    "        url = 'http://shapefiles.fews.net.s3.amazonaws.com/'+ region + date + '.zip'\n",
    "        zipFiles.append(downloadFile(url, path))\n",
    "\n",
    "    for file in zipFiles:\n",
    "        with zipfile.ZipFile(file,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(path)\n",
    "        os.remove(file)\n",
    "\n",
    "\n",
    "# Get all downloaded files \n",
    "\n",
    "def fileList(dataPath):\n",
    "    shpList=[]\n",
    "    for root, dirs, files in os.walk(dataPath):\n",
    "        for file in files:\n",
    "            if file.endswith(\".shp\"):\n",
    "                text = file.split('.')[0].split('_')\n",
    "                if len(text)>2:\n",
    "                    shpList.append({'path': os.path.join(root, file), 'stype': text[2], 'date': datetime.date(int(text[1][0:4]), int(text[1][4:6]), 1).isoformat(), 'region':text[0]})\n",
    "                else:\n",
    "                    shpList.append({'path': os.path.join(root, file), 'stype': text[1], 'date': datetime.date(int(text[0][-6::][0:4]),int(text[0][-6::][4:6]), 1).isoformat(), 'region':text[0][0:-6]})\n",
    "    return shpList\n",
    "\n",
    "# manage eachindividual shapefile aconditioning it to our require structure\n",
    "\n",
    "def manage_shp(path, stype, date, region):\n",
    "    '''\n",
    "    takes a shapefile modifies its structure and gives light into new structure\n",
    "    path: shapefile path\n",
    "    stype: ml1 or ml2\n",
    "    date: YYYY-MM-DD iso format\n",
    "    region: region\n",
    "    '''\n",
    "    newdata = gpd.GeoDataFrame(columns=['geometry', 'value', 'type','date','region'], crs = fiona.crs.from_epsg(4326))\n",
    "    dataset = gpd.read_file(path)\n",
    "    for index, rows in dataset.iterrows():\n",
    "        newdata.loc[index, 'geometry']=rows['geometry'].simplify(0.04,True)\n",
    "        newdata.loc[index,'value']=rows[0]\n",
    "        newdata.loc[index,'type']=stype\n",
    "        newdata.loc[index,'date']=date\n",
    "        newdata.loc[index,'region']=region\n",
    "    return newdata\n",
    "\n",
    "# S3 upload\n",
    "def s3Upload(outFile):\n",
    "    conn = tinys3.Connection(os.getenv('S3_ACCESS_KEY'),os.getenv('S3_SECRET_KEY'), tls=True, default_bucket=os.getenv('BUCKET'), endpoint=\"s3.amazonaws.com\")\n",
    "    # So we could skip the bucket parameter on every request\n",
    "    response = conn.upload(key=outFile, local_file=open(outFile,'rb'), public=True, close=True)\n",
    "    if response.status_code==200:\n",
    "        print('SUCCESS')\n",
    "    else:\n",
    "        print('UPLOAD PROCESS FAILURE STATUS CODE:' + str(response.status_code))\n",
    "        print(response.content)\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    1.- check for updates\n",
    "    2.- if there are updates, it will download everything in this folder\n",
    "    3._ it will download the current file in a separe place\n",
    "    4._ it will convert the new download to our requered format\n",
    "    5._ it will merge everything into one file \n",
    "    6.- it will zip it \n",
    "    7.- it will upload it to s3\n",
    "    '''\n",
    "    dfList=[]\n",
    "    outdir = 'dst'\n",
    "    outfile = outdir+'/fws.shp'\n",
    "    path = 'fws'\n",
    "    path2f = 'fws_original'\n",
    "    zipfilen = 'fws.zip'\n",
    "    args = {'q':'select date from fws order by date::date desc limit 1'}\n",
    "    url = 'https://wri-rw.carto.com/api/v2/sql'\n",
    "    \n",
    "    dataDate = requests.get(url, params=args).json()['rows'][0]['date'].split('-')\n",
    "    lastDate = datetime.date(int(dataDate[0]), int(dataDate[1]), int(dataDate[2]))\n",
    "    date = str(lastDate.year)+\"%02d\" % (lastDate.month+4)\n",
    "    pingUrl='http://shapefiles.fews.net.s3.amazonaws.com/west-africa'+ date +'.zip'\n",
    "    if requests.get(pingUrl).status_code!=200:\n",
    "        print('There is not new data for this date: ', date)\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "        os.mkdir(path2f)\n",
    "        getFiles(date, path)\n",
    "        os.mkdir(outdir)\n",
    "        shpList=fileList(path)\n",
    "        getFwsFile(path2f)\n",
    "\n",
    "        print(\"starting ...\")\n",
    "\n",
    "        for shp in shpList:\n",
    "            dfList.append(manage_shp(shp['path'],shp['stype'],shp['date'],shp['region']))\n",
    "\n",
    "        print(\"merging ...\")\n",
    "        dfList.append(gpd.read_file('fws_original/fws.shp'))\n",
    "        dst = pd.concat(dfList)\n",
    "        dst.to_file(outfile)\n",
    "\n",
    "        print(\"compressing ...\")\n",
    "        with zipfile.ZipFile(zipfilen, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            zipDir(outdir, zipf)\n",
    "\n",
    "        print(\"uploading ....\")\n",
    "\n",
    "        s3Upload(zipfilen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execution\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
